# Оценка качества

- Разделение выборок на обучающую и тестовую в соотношении 70% и 30% соответственно.

## Кросс-валидация

1. Повторение много раз
2. Посчет средней ошибки(mean error) $(X_{test},\ X_{train})$. Подразумевается знания среднеквадратичного отклонения (std)

### K-fold CV

*Идея*: разбить выборку на K-fold'ов. Поочередно кадлый fold нужно считать тестовой выборкой. На каждой итерации будет получен score и таким образом можно замерить статистический разброс score.

### leave-one-out

*Идея*: считать score на каждом отдельном примере, и аналогично оценивать статистические параметры score.

### Проблемы CV

*Пример*: бинарная оценка
- 1% - 0
- 99% - 1


#### Stratified K-fold

**Каким-то способом, непонятно каким, нужно убедиться что распределение по классам примерно одинаковое.**

## Проблема переобучения (Overfitting)

*Detecting*:
1. Проверить score на $X_{train}$
2. Сравнить со score на $X_{test}$

Если они сильно различаются, то вероятно вы попали в ситуацию overfitting. 

Оптимизация гиперпараметров алгоритма - тоже источник overfitting

Дополнительное разбиение на следующие части

- Обучающая выборка $X_{train}$
- Тестовая выборка для оптимизации гиперпараметров $X_{dev}$
- Тестовая выборка для оценки качества алгоритма $X_{testq}$

```py
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
```

learning curve - функционал качества